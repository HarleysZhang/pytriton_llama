from dataclasses import dataclass,field, fields
from typing import Any, Dict, List, Optional, Tuple, Union
import os, json


@dataclass
class LlamaConfig:
    architectures: List[str] = field(default_factory=lambda: ["LlamaForCausalLM"])
    attention_bias: bool = False
    attention_dropout: float = 0.0
    bos_token_id: Optional[int] = None
    eos_token_id: Optional[int] = None
    head_dim: Optional[int] = None
    hidden_act: str = "silu"
    
    initializer_range: float = 0.02
    
    hidden_size: int = 2048  # 默认值调整为2048，保持一致性
    intermediate_size: int = 8192
    max_position_embeddings: Optional[int] = None
    mlp_bias: bool = False
    model_type: str = "llama"
    num_heads: Optional[int] = 32  # 设置合理的默认值
    num_layers: Optional[int] = 32
    num_kv_heads: Optional[int] = None
    pretraining_tp: int = 1
    rms_norm_eps: float = 1e-5
    rope_scaling: Optional[Dict[str, Any]] = None
    rope_theta: float = 10000.0
    tie_word_embeddings: bool = True
    torch_dtype: str = "bfloat16"
    transformers_version: Optional[str] = None
    use_cache: bool = True
    vocab_size: Optional[int] = 32064

    _name_or_path: Optional[str] = None
    max_batch_size: int = 64
    max_seq_len: int = 2048
    device: str = "cuda"

    def __post_init__(self):
        if self.num_heads and self.hidden_size:
            self.head_dim = self.hidden_size // self.num_heads
        else:
            self.head_dim = None  # 或者设置一个默认值，例如 64

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'LlamaConfig':
        # 定义字段映射
        key_mappings = {
            'num_attention_heads': 'num_heads',
            'num_hidden_layers': 'num_layers',
            'num_key_value_heads': 'num_kv_heads',
        }

        # 创建一个复制的字典，以避免修改原始数据
        data_copy = data.copy()

        # 应用字段映射
        for old_key, new_key in key_mappings.items():
            if old_key in data_copy:
                data_copy[new_key] = data_copy.pop(old_key)

        # 获取 LlamaConfig 类中所有的字段名
        valid_keys = {f.name for f in fields(cls)}

        # 过滤掉不在 LlamaConfig 中定义的键
        data_filtered = {k: v for k, v in data_copy.items() if k in valid_keys}

        # 设置默认值，确保所有必要字段都有值
        defaults = {
            'architectures': ["LlamaForCausalLM"],
            'attention_bias': False,
            'attention_dropout': 0.0,
            'bos_token_id': None,
            'eos_token_id': None,
            'hidden_act': "silu",
            'initializer_range': 0.02,
            'hidden_size': 2048,
            'intermediate_size': 8192,
            'max_position_embeddings': None,
            'mlp_bias': False,
            'model_type': "llama",
            'num_heads': 32,
            'num_layers': 32,
            'num_kv_heads': None,
            'pretraining_tp': 1,
            'rms_norm_eps': 1e-5,
            'rope_scaling': None,
            'rope_theta': 10000.0,
            'tie_word_embeddings': True,
            'torch_dtype': "bfloat16",
            'transformers_version': None,
            'use_cache': True,
            'vocab_size': 32064,
            '_name_or_path': None,
            'max_batch_size': 64,
            'max_seq_len': 2048,
            'device': "cuda",
        }

        # 更新缺失的字段
        for key, value in defaults.items():
            data_filtered.setdefault(key, value)

        return cls(**data_filtered)
    
@dataclass
class Qwen2Config:
    max_batch_size: int = 4
    max_seq_len: int = 2048

    architectures: Optional[list] = None
    attention_dropout: float = 0.0
    bos_token_id: Optional[int] = 151643
    eos_token_id: Optional[int] = 151645
    hidden_act: str = "silu"
    
    # dim: Optional[int] = None
    initializer_range: float = 0.02
    
    # 模型隐藏层大小, Qwen2.5-1.5B-Instruct
    hidden_size: Optional[int] = 1536
    intermediate_size: Optional[int] = 8960
    max_position_embeddings: Optional[int] = 32768

    mlp_bias: bool = False
    model_type: str = "qwen2"
    # 注意力头数，也就是 q heads 头数
    num_heads: Optional[int] = 12
    # 解码层数
    num_layers: Optional[int] = 28
    # 使用了 GQA 技术的 kv heads 头数
    num_kv_heads: Optional[int] = 2

    rms_norm_eps: float = 1e-6
    rope_scaling: Optional[Dict[str, Any]] = None
    rope_theta: float = 1000000.0

    torch_dtype: str = "bfloat16"
    transformers_version: Optional[str] = "4.43.1"
    use_cache: bool = True
    vocab_size: Optional[int] = 151936

    tie_word_embeddings: bool = False
    use_sliding_window: bool = False
    sliding_window: int = 4096
    max_window_layers: int = 21
    device: str = "cuda"

    def __init__(self, config_dict: Optional[Dict[str, Any]] = None, **kwargs):
        self.sliding_window = self.sliding_window if self.use_sliding_window else None

        # 首先，设置默认属性值
        for field_name, field_def in self.__dataclass_fields__.items():
            setattr(self, field_name, field_def.default)

        # 如果提供了 config_dict，从中更新属性
        if config_dict is not None:
            for key, value in config_dict.items():
                # 处理名称映射
                if key == 'num_attention_heads':
                    self.num_heads = value
                elif key == 'num_hidden_layers':
                    self.num_layers = value
                elif key == 'num_key_value_heads':
                    self.num_kv_heads = value
                elif key == 'max_length':
                    self.max_seq_len = value
                else:
                    setattr(self, key, value)

        # 处理额外的关键字参数
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                # 如果属性不存在，可以选择存储在 extra_args 中，或者直接添加
                setattr(self, key, value)
        self.head_dim = self.hidden_size // self.num_heads
    
@dataclass
class CLIPVisionConfig():
    """
    This is the configuration class to store the configuration of a [`CLIPVisionModel`]. It is used to instantiate a
    CLIP vision encoder according to the specified arguments, defining the model architecture. Instantiating a
    configuration with the defaults will yield a similar configuration to that of the vision encoder of the CLIP
    [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture.
    """
    hidden_size: int = 768,
    intermediate_size: int = 3072,
    projection_dim: int = 512,
    num_layers: int = 12, # encoder_layer 层数
    num_heads: int = 12,  # attention 模块的头数目
    num_channels: int = 3,
    image_size: int = 224,
    patch_size: int = 32,
    hidden_act: int = "quick_gelu",
    layer_norm_eps: int = 1e-5,
    attention_dropout: int = 0.0,
    initializer_range: int = 0.02,
    initializer_factor: int = 1.0,

    model_type: str = "clip_vision_model"

    def __init__(self, config_dict: Optional[Dict[str, Any]] = None, **kwargs):
        self.sliding_window = self.sliding_window if self.use_sliding_window else None

        # 首先，设置默认属性值
        for field_name, field_def in self.__dataclass_fields__.items():
            setattr(self, field_name, field_def.default)

        # 如果提供了 config_dict，从中更新属性, clip 模型的配置文件包含 text 和 vision 配置
        if config_dict is not None:
            # get the vision config dict if we are loading from CLIPConfig
            if config_dict.get("model_type") == "clip":
                config_dict = config_dict["vision_config"]
            else:
                print("Error! clip model config file not include vision config!")

            for key, value in config_dict.items():
                # 处理名称映射
                if key == 'num_attention_heads':
                    self.num_heads = value
                elif key == 'num_hidden_layers':
                    self.num_layers = value
                elif key == 'num_key_value_heads':
                    self.num_kv_heads = value
                else:
                    setattr(self, key, value)

        # 处理额外的关键字参数
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                # 如果属性不存在，可以选择存储在 extra_args 中，或者直接添加
                setattr(self, key, value)
        self.head_dim = self.hidden_size // self.num_heads


@dataclass
class VisionConfig:
    hidden_size: int
    image_size: int
    intermediate_size: int
    model_type: str
    num_attention_heads: int
    num_hidden_layers: int
    patch_size: int
    projection_dim: int
    vocab_size: int

    @staticmethod
    def from_dict(data: Dict[str, Any]) -> 'VisionConfig':
        return VisionConfig(
            hidden_size=data.get("hidden_size", 768),
            image_size=data.get("image_size", 224),
            intermediate_size=data.get("intermediate_size", 3072),
            model_type=data.get("model_type", "clip_vision_model"),
            num_attention_heads=data.get("num_attention_heads", 12),
            num_hidden_layers=data.get("num_hidden_layers", 12),
            patch_size=data.get("patch_size", 16),
            projection_dim=data.get("projection_dim", 768),
            vocab_size=data.get("vocab_size", 1000)
        )

@dataclass
class LlavaConfig:
    architectures: List[str]
    ignore_index: int
    image_token_index: int
    model_type: str
    pad_token_id: int
    projector_hidden_act: str
    text_config: LlamaConfig
    tie_word_embeddings: bool
    torch_dtype: str
    transformers_version: str
    vision_config: VisionConfig
    vision_feature_layer: int
    vision_feature_select_strategy: str
    vocab_size: int
    image_seq_length: int = 576
    max_batch_size: int = 64
    max_seq_len: int = 2048
    device: str = "cuda"
    
    @staticmethod
    def from_dict(data: Dict[str, Any]) -> 'LlavaConfig':
        text_cfg = LlamaConfig.from_dict(data.get("text_config", {}))
        vision_cfg = VisionConfig.from_dict(data.get("vision_config", {}))
        return LlavaConfig(
            architectures=data.get("architectures", []),
            ignore_index=data.get("ignore_index", -100),
            image_token_index=data.get("image_token_index", 32000),
            model_type=data.get("model_type", "llava"),
            pad_token_id=data.get("pad_token_id", 32001),
            projector_hidden_act=data.get("projector_hidden_act", "gelu"),
            text_config=text_cfg,
            tie_word_embeddings=data.get("tie_word_embeddings", False),
            torch_dtype=data.get("torch_dtype", "float16"),
            transformers_version=data.get("transformers_version", "4.36.0.dev0"),
            vision_config=vision_cfg,
            vision_feature_layer=data.get("vision_feature_layer", -2),
            vision_feature_select_strategy=data.get("vision_feature_select_strategy", "default"),
            vocab_size=data.get("vocab_size", 32064)
        )

    @classmethod
    def from_json(cls, json_path: str) -> 'LlavaConfig':
        with open(json_path, 'r') as f:
            data = json.load(f)
        return cls.from_dict(data)

    @classmethod
    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):
        """类方法，用于从指定的 JSON 文件中读取数据并将其解析为字典对象"""
        with open(json_file, "r", encoding="utf-8") as reader:
            text = reader.read()
        
        # NOTE: 使用 json.loads 函数将读取到的 JSON 格式字符串解析为 Python 字典对象
        return json.loads(text)
