import unittest
import torch
import re

# 假设 IMAGE_TOKEN_INDEX 为 1000
IMAGE_TOKEN_INDEX = 32000

class MockTokenizer:
    def __init__(self, bos_token_id=101, eos_token_id=102):
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
    
    def __call__(self, text):
        # 简单模拟分词器，将每个字符转换为其 ASCII 值
        # 并在句首添加 BOS token，如果需要
        input_ids = []
        if text.startswith('<BOS>'):
            input_ids.append(self.bos_token_id)
            text = text[5:]
        for char in text:
            input_ids.append(ord(char))
        return MockEncoding(input_ids)
    
class MockEncoding:
    def __init__(self, input_ids):
        self.input_ids = input_ids

def tokenizer_image_token(
    prompt, 
    tokenizer, 
    image_token_index=IMAGE_TOKEN_INDEX, 
    return_tensors=None
):
    """
    处理包含特殊标记 <image> 的文本提示, 将其转换为相应的 token 序列，并在 <image> 位置插入指定的图像 token 索引。
    
    参数:
        prompt (str): 包含 <image> 标记的文本。
        tokenizer: 分词器对象，需支持调用 tokenizer(chunk).input_ids。
        image_token_index (int): 用于替换 <image> 标记的图像 token 索引。
        return_tensors (str, optional): 指定返回的张量类型，例如 'pt' 表示 PyTorch 张量。
    
    返回:
        list 或 torch.Tensor: 生成的 token 序列。
    """
    # 使用正则表达式分割，移除 '<image>' 前的空格，但保留后的空格
    prompt_chunks = re.split(r'\s?<image>', prompt)
    # 不过滤空片段，以处理多个连续的 '<image>' 标记
    token_chunks = [tokenizer(chunk).input_ids for chunk in prompt_chunks]
    
    input_ids = []
    offset = 0
    # 检查第一个片段是否以 BOS token 开始
    if len(token_chunks) > 0 and len(token_chunks[0]) > 0 and token_chunks[0][0] == tokenizer.bos_token_id:
        offset = 1
        input_ids.append(token_chunks[0][0])
    
    # 插入图像 token
    for i, chunk in enumerate(token_chunks):
        # 添加当前片段的 token，跳过 BOS token（如果已添加）
        input_ids.extend(chunk[offset:])
        offset = 0  # 仅适用于第一个片段
        # 如果不是最后一个片段，插入图像 token
        if i < len(token_chunks) - 1:
            input_ids.append(image_token_index)
    
    if return_tensors is not None:
        if return_tensors == 'pt':
            return torch.tensor(input_ids, dtype=torch.long)
        raise ValueError(f'Unsupported tensor type: {return_tensors}')
    return input_ids

class TestTokenizerImageToken(unittest.TestCase):
    def setUp(self):
        self.tokenizer = MockTokenizer()
    
    def test_single_image(self):
        prompt = "Hello <image> world."
        # "Hello" -> [72, 101, 108, 108, 111]
        # " world." -> [32, 119, 111, 114, 108, 100, 46]
        # After insertion: [72,101,108,108,111,1000,32,119,111,114,108,100,46]
        expected_input_ids = [
            ord('H'), ord('e'), ord('l'), ord('l'), ord('o'), 
            IMAGE_TOKEN_INDEX, 
            ord(' '), ord('w'), ord('o'), ord('r'), ord('l'), ord('d'), ord('.')
        ]
        result = tokenizer_image_token(prompt, self.tokenizer)
        self.assertEqual(result, expected_input_ids)
    
    def test_multiple_images(self):
        prompt = "A cat <image> is sitting <image> on the mat."
        # "A cat" -> [65, 32, 99, 97, 116]
        # " is sitting" -> [32, 105, 115, 32, 115, 105, 116, 116, 105, 110, 103]
        # " on the mat." -> [32, 111, 110, 32, 116, 104, 101, 32, 109, 97, 116, 46]
        # After insertion: [65,32,99,97,116,1000,32,105,115,32,115,105,116,116,105,110,103,1000,32,111,110,32,116,104,101,32,109,97,116,46]
        expected_input_ids = [
            ord('A'), ord(' '), ord('c'), ord('a'), ord('t'), 
            IMAGE_TOKEN_INDEX, 
            ord(' '), ord('i'), ord('s'), ord(' '), ord('s'), ord('i'), ord('t'), ord('t'), ord('i'), ord('n'), ord('g'), 
            IMAGE_TOKEN_INDEX, 
            ord(' '), ord('o'), ord('n'), ord(' '), ord('t'), ord('h'), ord('e'), ord(' '), ord('m'), ord('a'), ord('t'), ord('.')
        ]
        result = tokenizer_image_token(prompt, self.tokenizer)
        self.assertEqual(result, expected_input_ids)
    
    def test_no_image(self):
        prompt = "This is a text without images."
        # "This is a text without images." -> [84, 104, 105, 115, 32, 105, 115, 32, 97, 32, 116, 101, 120, 116, 32, 119, 105, 116, 104, 111, 117, 116, 32, 105, 109, 97, 103, 101, 115, 46]
        expected_input_ids = [
            ord('T'), ord('h'), ord('i'), ord('s'), ord(' '),
            ord('i'), ord('s'), ord(' '), ord('a'), ord(' '),
            ord('t'), ord('e'), ord('x'), ord('t'), ord(' '),
            ord('w'), ord('i'), ord('t'), ord('h'), ord('o'), ord('u'), ord('t'), ord(' '),
            ord('i'), ord('m'), ord('a'), ord('g'), ord('e'), ord('s'), ord('.')
        ]
        result = tokenizer_image_token(prompt, self.tokenizer)
        self.assertEqual(result, expected_input_ids)
    
    def test_leading_bos_token(self):
        prompt = "<BOS>Start <image> end."
        # "<BOS>Start" -> [101, 83, 116, 97, 114, 116]
        # " end." -> [32, 101, 110, 100, 46]
        # After insertion: [101,83,116,97,114,116,1000,32,101,110,100,46]
        expected_input_ids = [
            101,  # BOS token
            ord('S'), ord('t'), ord('a'), ord('r'), ord('t'), 
            IMAGE_TOKEN_INDEX, 
            ord(' '), ord('e'), ord('n'), ord('d'), ord('.')
        ]
        print("expected_input_ids ", expected_input_ids)
        result = tokenizer_image_token(prompt, self.tokenizer)
        self.assertEqual(result, expected_input_ids)
    
    def test_consecutive_images(self):
        prompt = "Image1 <image><image> Image2."
        # "Image1" -> [73, 109, 97, 103, 101, 49]
        # "" -> []
        # " Image2." -> [32, 73, 109, 97, 103, 101, 50, 46]
        # After insertion: [73,109,97,103,101,49,1000,1000,32,73,109,97,103,101,50,46]
        expected_input_ids = [
            ord('I'), ord('m'), ord('a'), ord('g'), ord('e'), ord('1'), 
            IMAGE_TOKEN_INDEX, 
            IMAGE_TOKEN_INDEX, 
            ord(' '), ord('I'), ord('m'), ord('a'), ord('g'), ord('e'), ord('2'), ord('.')
        ]
        result = tokenizer_image_token(prompt, self.tokenizer)
        self.assertEqual(result, expected_input_ids)
    
    def test_return_tensors_pt(self):
        prompt = "Hello <image> world."
        # [72,101,108,108,111,1000,32,119,111,114,108,100,46]
        expected_tensor = torch.tensor([
            ord('H'), ord('e'), ord('l'), ord('l'), ord('o'), 
            IMAGE_TOKEN_INDEX, 
            ord(' '), ord('w'), ord('o'), ord('r'), ord('l'), ord('d'), ord('.')
        ], dtype=torch.long)
        result = tokenizer_image_token(prompt, self.tokenizer, return_tensors='pt')
        self.assertTrue(torch.equal(result, expected_tensor))
    
    def test_return_tensors_unsupported(self):
        prompt = "Hello <image> world."
        with self.assertRaises(ValueError):
            tokenizer_image_token(prompt, self.tokenizer, return_tensors='np')

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False)